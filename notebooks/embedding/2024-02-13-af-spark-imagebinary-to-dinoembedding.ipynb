{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/afischer/snakeclef-2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/21 00:00:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/21 00:00:16 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image_path: string (nullable = true)\n",
      " |-- path: string (nullable = true)\n",
      " |-- folder_name: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- binomial_name: string (nullable = true)\n",
      " |-- file_name: string (nullable = true)\n",
      " |-- data: binary (nullable = true)\n",
      " |-- observation_id: integer (nullable = true)\n",
      " |-- endemic: boolean (nullable = true)\n",
      " |-- code: string (nullable = true)\n",
      " |-- class_id: integer (nullable = true)\n",
      " |-- subset: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68495"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /home/afischer/snakeclef-2024/\n",
    "from snakeclef.utils import get_spark\n",
    "\n",
    "# https://knowledge.informatica.com/s/article/000196886?language=en_US\n",
    "# The vectorized reader will run out of memory (8gb) with the default batch size, so\n",
    "# this is one way of handling the issue. This is likely due to the fact that the data\n",
    "# column is so damn big, and treated as binary data instead of something like a string.\n",
    "# We might also be able to avoid this if we don't cache the fields into memory, but this\n",
    "# this needs to be validated by hand. \n",
    "spark = get_spark(**{\n",
    "    # \"spark.sql.parquet.columnarReaderBatchSize\": 512,\n",
    "    \"spark.sql.parquet.enableVectorizedReader\": False, \n",
    "})\n",
    "\n",
    "size = 'small' # small, medium, large\n",
    "gcs_parquet_path = \"gs://dsgt-clef-snakeclef-2024/data/parquet_files/\"\n",
    "input_folder = f\"SnakeCLEF2023-train-{size}_size/\"\n",
    "\n",
    "df = spark.read.parquet(gcs_parquet_path+input_folder)\n",
    "df.printSchema()\n",
    "# df.show(1, vertical=True, truncate=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.types import BinaryType, ArrayType, FloatType\n",
    "import io\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def make_predict_fn():\n",
    "    \"\"\"Return PredictBatchFunction\"\"\"\n",
    "    from transformers import AutoImageProcessor, AutoModel\n",
    "    processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "    model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "\n",
    "    def predict(inputs: np.ndarray) -> np.ndarray:\n",
    "        # print('inputs:')\n",
    "        # print(type(inputs))\n",
    "        # print(inputs.shape)\n",
    "\n",
    "        images = [Image.open(io.BytesIO(input)) for input in inputs]\n",
    "        # print('images:')\n",
    "        # print(type(images))\n",
    "        # print(images)\n",
    "\n",
    "        model_inputs = processor(images=images, return_tensors=\"pt\")\n",
    "        # print('model_inputs:')\n",
    "        # print(type(model_inputs))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # print('start modeling')\n",
    "            outputs = model(**model_inputs)\n",
    "            # print('outputs')\n",
    "            # print(outputs)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        # print('last_hidden_states:')\n",
    "        # print(type(last_hidden_states))\n",
    "        # print(last_hidden_states.shape)\n",
    "\n",
    "        numpy_array = last_hidden_states.numpy()\n",
    "        # Reshape the array\n",
    "        new_shape = numpy_array.shape[:-2] + (-1,)\n",
    "        numpy_array = numpy_array.reshape(new_shape)\n",
    "\n",
    "        # print('numpy_array:')\n",
    "        # print(type(numpy_array))\n",
    "        # print(numpy_array.shape)\n",
    "\n",
    "        return numpy_array\n",
    "\n",
    "    return predict\n",
    "    \n",
    "# batch prediction UDF\n",
    "apply_dino_pbudf = predict_batch_udf(\n",
    "    make_predict_fn = make_predict_fn,\n",
    "    return_type=ArrayType(FloatType()),\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image_path: string (nullable = true)\n",
      " |-- path: string (nullable = true)\n",
      " |-- folder_name: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- binomial_name: string (nullable = true)\n",
      " |-- file_name: string (nullable = true)\n",
      " |-- data: binary (nullable = true)\n",
      " |-- observation_id: integer (nullable = true)\n",
      " |-- endemic: boolean (nullable = true)\n",
      " |-- code: string (nullable = true)\n",
      " |-- class_id: integer (nullable = true)\n",
      " |-- subset: string (nullable = true)\n",
      " |-- transformed_image: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the UDF to transform images\n",
    "df_transformed = df.limit(24).withColumn(\"transformed_image\", apply_dino_pbudf(df[\"data\"]))\n",
    "\n",
    "df_transformed.printSchema()\n",
    "# df_transformed.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_folder = f'DINOv2-embeddings-{size}_size/'\n",
    "df_transformed.write.mode(\"overwrite\").parquet(gcs_parquet_path+output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image_path: string (nullable = true)\n",
      " |-- path: string (nullable = true)\n",
      " |-- folder_name: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- binomial_name: string (nullable = true)\n",
      " |-- file_name: string (nullable = true)\n",
      " |-- data: binary (nullable = true)\n",
      " |-- observation_id: integer (nullable = true)\n",
      " |-- endemic: boolean (nullable = true)\n",
      " |-- code: string (nullable = true)\n",
      " |-- class_id: integer (nullable = true)\n",
      " |-- subset: string (nullable = true)\n",
      " |-- transformed_image: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check outputs\n",
    "\n",
    "output_df = spark.read.parquet(gcs_parquet_path+output_folder)\n",
    "output_df.printSchema()\n",
    "# output_df.show(1, vertical=True, truncate=True)\n",
    "output_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "output_df.filter(col(\"transformed_image\").isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------\n",
      " image_path        | 1993/Phrynonax_po... \n",
      " path              | /SnakeCLEF2023-sm... \n",
      " folder_name       | SnakeCLEF2023-sma... \n",
      " year              | 1993                 \n",
      " binomial_name     | Phrynonax_polylepis  \n",
      " file_name         | 102870166.jpg        \n",
      " data              | [FF D8 FF E0 00 1... \n",
      " observation_id    | 64030606             \n",
      " endemic           | false                \n",
      " code              | EC                   \n",
      " class_id          | 1287                 \n",
      " subset            | train                \n",
      " transformed_image | [2.94488, -1.4730... \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
