{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image to Parquet\n",
    "\n",
    "Convert images to binary and save them into a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/25 17:33:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fd9084d1630>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Images2Parquet\").getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Base directory using pathlib\n",
    "curr_dir = Path(os.getcwd())\n",
    "base_dir = curr_dir.parents[1] / \"data\" / \"SnakeCLEF2023-small_size\" / \"2023\"\n",
    "num_folders = 20\n",
    "\n",
    "# Ensure base directory exists\n",
    "if not base_dir.is_dir():\n",
    "    raise FileNotFoundError(f\"Base directory {base_dir} does not exist.\")\n",
    "\n",
    "# Getting subfolders\n",
    "subfolders = sorted([f.name for f in base_dir.iterdir() if f.is_dir()])[:num_folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for the DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, BinaryType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"path\", StringType(), True),\n",
    "    StructField(\"image_binary_data\", BinaryType(), True)\n",
    "])\n",
    "\n",
    "# Function to convert image to binary\n",
    "def image_to_binary(image_path):\n",
    "    with open(image_path, 'rb') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty RDD\n",
    "image_rdd = spark.sparkContext.emptyRDD()\n",
    "\n",
    "# Loop through subfolders and process images\n",
    "for _, folder in enumerate(subfolders):\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    for img_name in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "        binary_data = image_to_binary(img_path)\n",
    "        image_rdd = image_rdd.union(spark.sparkContext.parallelize([(img_path, binary_data)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                path|   image_binary_data|\n",
      "+--------------------+--------------------+\n",
      "|/home/mgustine/sn...|[FF D8 FF E0 00 1...|\n",
      "|/home/mgustine/sn...|[FF D8 FF E0 00 1...|\n",
      "|/home/mgustine/sn...|[FF D8 FF E0 00 1...|\n",
      "|/home/mgustine/sn...|[FF D8 FF E0 00 1...|\n",
      "|/home/mgustine/sn...|[FF D8 FF E0 00 1...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:======================================================>(285 + 3) / 288]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in image_df: 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert RDD to DataFrame\n",
    "image_df = spark.createDataFrame(image_rdd, schema)\n",
    "\n",
    "# Show the first few rows of image_df\n",
    "image_df.show(n=5)\n",
    "\n",
    "# Count the number of rows in image_df\n",
    "row_count = image_df.count()\n",
    "print(f\"Number of rows in image_df: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory for parquet_files folder\n",
    "data_dir = Path(os.getcwd()).parents[1] / \"data\"\n",
    "\n",
    "# Create \"parquet_files\" directory if it doesn't exist\n",
    "parquet_dir = data_dir / \"parquet_files\"\n",
    "os.makedirs(parquet_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Path to the Parquet file\n",
    "parquet_file_path = parquet_dir / \"images_data.parquet\"\n",
    "\n",
    "# Save the DataFrame as a Parquet file\n",
    "image_df.write.mode(\"overwrite\").parquet(str(parquet_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Parquet file: 2897022 bytes\n"
     ]
    }
   ],
   "source": [
    "def get_size_of_parquet(dir_path):\n",
    "    total_size = 0\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            total_size += os.path.getsize(os.path.join(root, file))\n",
    "    return total_size\n",
    "\n",
    "# Get the size of the Parquet file (directory)\n",
    "parquet_size = get_size_of_parquet(parquet_file_path)\n",
    "print(f\"Size of Parquet file: {parquet_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GCS path\n",
    "gcs_path = \"gs://dsgt-clef-snakeclef-2024/data/parquet_files/image_data\"\n",
    "\n",
    "# Write the DataFrame to GCS\n",
    "image_df.write.mode(\"overwrite\").parquet(gcs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark jars directory does not exist: /usr/local/lib/python3.10/dist-packages/pyspark/jars\n"
     ]
    }
   ],
   "source": [
    "import site\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "lib_dir=\"/home/username/.local/lib/python3.8/site-packages/pyspark/jars\"\n",
    "\n",
    "site_packages = site.getsitepackages()[0]\n",
    "lib_dir = os.path.join(site_packages, \"pyspark\", \"jars\")\n",
    "if not os.path.exists(lib_dir):\n",
    "    print(f\"PySpark jars directory does not exist: {lib_dir}\")\n",
    "else:\n",
    "    subprocess.run([\"wget\", \"https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar\", \"-P\", lib_dir])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
