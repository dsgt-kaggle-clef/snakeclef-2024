{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image to Parquet\n",
    "Convert images to binary and save them into a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import site\n",
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gcs_connector_jar() -> str:\n",
    "    # Assuming the JAR is installed in the user site-packages of PySpark\n",
    "    user_site_packages = site.getusersitepackages()\n",
    "    jars_dir = Path(user_site_packages) / \"pyspark\" / \"jars\"\n",
    "    # Search for the GCS connector JAR\n",
    "    jar = [jar for jar in jars_dir.glob(\"gcs-connector-hadoop3-*.jar\")]\n",
    "    return str(jar[0])\n",
    "\n",
    "\n",
    "gcs_connector_jar = get_gcs_connector_jar()\n",
    "# Initialize Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Image2Parquet\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.jars\", gcs_connector_jar)\n",
    "    .getOrCreate()\n",
    ")\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gcs_connector_jar() -> str:\n",
    "    # Assuming the JAR is installed in the user site-packages of PySpark\n",
    "    user_site_packages = site.getusersitepackages()\n",
    "    jars_dir = Path(user_site_packages) / \"pyspark\" / \"jars\"\n",
    "    # Search for the GCS connector JAR\n",
    "    jar = [jar for jar in jars_dir.glob(\"gcs-connector-hadoop3-*.jar\")]\n",
    "    return str(jar[0])\n",
    "\n",
    "\n",
    "gcs_connector_jar = get_gcs_connector_jar()\n",
    "gcs_connector_jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hadoop configurations for GCS\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\n",
    "    \"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\"\n",
    ")\n",
    "sc._jsc.hadoopConfiguration().set(\n",
    "    \"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Base directory using pathlib\n",
    "curr_dir = Path(os.getcwd())\n",
    "base_dir = curr_dir.parents[1]\n",
    "data_dir = base_dir / \"data\" / \"SnakeCLEF2023-small_size\" / \"2023\"\n",
    "num_folders = 20\n",
    "\n",
    "# Ensure base directory exists\n",
    "if not data_dir.is_dir():\n",
    "    raise FileNotFoundError(f\"Data directory {data_dir} does not exist.\")\n",
    "\n",
    "# Getting subfolders\n",
    "subfolders = sorted([f.name for f in data_dir.iterdir() if f.is_dir()])[:num_folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for the DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, BinaryType, StringType\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"path\", StringType(), True),\n",
    "        StructField(\"folder_name\", StringType(), True),\n",
    "        StructField(\"year\", StringType(), True),\n",
    "        StructField(\"binomial_name\", StringType(), True),\n",
    "        StructField(\"file_name\", StringType(), True),\n",
    "        StructField(\"image_binary_data\", BinaryType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Function to convert image to binary\n",
    "def image_to_binary(image_path):\n",
    "    with open(image_path, \"rb\") as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty RDD\n",
    "image_rdd = spark.sparkContext.emptyRDD()\n",
    "\n",
    "# Loop through subfolders and process images\n",
    "for folder in subfolders:\n",
    "    folder_path = data_dir / folder\n",
    "    for img_name in os.listdir(folder_path):\n",
    "        img_path = folder_path / img_name\n",
    "        relative_path = img_path.relative_to(base_dir)  # Get relative path\n",
    "        relative_path = str(relative_path).split(\"data/\")[-1]\n",
    "        folder_name = relative_path.split(\"/\")[0]\n",
    "        year = relative_path.split(\"/\")[1]\n",
    "        binomial_name = relative_path.split(\"/\")[2]\n",
    "        file_name = relative_path.split(\"/\")[-1]\n",
    "        # print(f\"{folder_name}, {year}, {binomial_name}, {file_name}\")\n",
    "        binary_data = image_to_binary(str(img_path))\n",
    "        image_rdd = image_rdd.union(\n",
    "            spark.sparkContext.parallelize(\n",
    "                [\n",
    "                    (\n",
    "                        relative_path,\n",
    "                        folder_name,\n",
    "                        year,\n",
    "                        binomial_name,\n",
    "                        file_name,\n",
    "                        binary_data,\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert RDD to DataFrame\n",
    "image_df = spark.createDataFrame(image_rdd, schema)\n",
    "\n",
    "# Show the first few rows of image_df\n",
    "image_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory for parquet_files folder\n",
    "data_dir = Path(os.getcwd()).parents[1] / \"data\"\n",
    "\n",
    "# Create \"parquet_files\" directory if it doesn't exist\n",
    "parquet_dir = data_dir / \"parquet_files\"\n",
    "os.makedirs(parquet_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Parquet file\n",
    "parquet_file_path = parquet_dir / \"images_data.parquet\"\n",
    "\n",
    "# Save the DataFrame as a Parquet file\n",
    "image_df.write.mode(\"overwrite\").parquet(str(parquet_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_of_parquet(dir_path):\n",
    "    total_size = 0\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            total_size += os.path.getsize(os.path.join(root, file))\n",
    "    return total_size\n",
    "\n",
    "\n",
    "# Get the size of the Parquet file (directory)\n",
    "parquet_size = get_size_of_parquet(parquet_file_path)\n",
    "print(f\"Size of Parquet file: {parquet_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GCS path\n",
    "gcs_path = \"gs://dsgt-clef-snakeclef-2024/data/parquet_files/image_data\"\n",
    "\n",
    "# Write the DataFrame to GCS\n",
    "image_df.write.mode(\"overwrite\").parquet(gcs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DataFrame from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/03 18:13:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://snakeclef-dev.us-central1-a.c.dsgt-clef-2024.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb681359450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from snakeclef.utils import get_spark\n",
    "\n",
    "spark = get_spark()\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/03 18:13:25 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://dsgt-clef-snakeclef-2024/data/parquet_files/image_data.\n",
      "org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"gs\"\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o35.parquet.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"gs\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m gcs_parquet_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://dsgt-clef-snakeclef-2024/data/parquet_files/image_data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Read the Parquet file into a DataFrame\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgcs_parquet_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Show the data (for example, first few rows)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m df\u001b[38;5;241m.\u001b[39mshow(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    542\u001b[0m )\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o35.parquet.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"gs\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# Define the GCS path to the Parquet file\n",
    "gcs_parquet_path = \"gs://dsgt-clef-snakeclef-2024/data/parquet_files/image_data\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = spark.read.parquet(gcs_parquet_path)\n",
    "\n",
    "# Show the data (for example, first few rows)\n",
    "df.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def display_images_from_binary(image_data_list, binomial_names, grid_size=(3, 3)):\n",
    "    \"\"\"\n",
    "    Display images in a grid with binomial names as labels.\n",
    "\n",
    "    :param image_data_list: List of binary image data.\n",
    "    :param binomial_names: List of binomial names corresponding to each image.\n",
    "    :param grid_size: Tuple (rows, cols) representing the grid size.\n",
    "    \"\"\"\n",
    "    # Unpack the number of rows and columns for the grid\n",
    "    rows, cols = grid_size\n",
    "\n",
    "    # Create a matplotlib subplot with the specified grid size\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12, 12), dpi=80)\n",
    "\n",
    "    # Flatten the axes array for easy iteration if it's 2D\n",
    "    axes = axes.flatten() if isinstance(axes, np.ndarray) else [axes]\n",
    "\n",
    "    for ax, binary_data, name in zip(axes, image_data_list, binomial_names):\n",
    "        # Convert binary data to an image and display it\n",
    "        image = Image.open(io.BytesIO(binary_data))\n",
    "        ax.imshow(image)\n",
    "        name = name.replace(\"_\", \" \")\n",
    "        ax.set_xlabel(name)  # Set the binomial name as xlabel\n",
    "        ax.xaxis.label.set_size(14)  # Set the font size for the xlabel\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect binary image data from DataFrame\n",
    "rows, cols = 3, 3\n",
    "image_data_list = [row[\"image_binary_data\"] for row in df.limit(rows * cols).collect()]\n",
    "binomial_names = [row[\"binomial_name\"] for row in df.limit(rows * cols).collect()]\n",
    "\n",
    "# Display the images in a grid with binomial names\n",
    "display_images_from_binary(image_data_list, binomial_names, grid_size=(3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join image DataFrame with Metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of stored filed in cloud bucket\n",
    "! gcloud storage ls gs://dsgt-clef-snakeclef-2024/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for the Train Metadata CSV files\n",
    "train_meta_hm_connector = (\n",
    "    \"gs://dsgt-clef-snakeclef-2024/raw/SnakeCLEF2023-TrainMetadata-HM.csv\"\n",
    ")\n",
    "train_meta_inat_connector = (\n",
    "    \"gs://dsgt-clef-snakeclef-2024/raw/SnakeCLEF2023-TrainMetadata-iNat.csv\"\n",
    ")\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df_train_meta_hm = spark.read.option(\"header\", True).csv(train_meta_hm_connector)\n",
    "df_train_meta_inat = spark.read.option(\"header\", True).csv(train_meta_inat_connector)\n",
    "\n",
    "# Show the data (for example, first few rows)\n",
    "df_train_meta_hm.show(n=3)\n",
    "df_train_meta_inat.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# df_hm_species = df_train_meta_hm.where(F.col(\"binomial_name\") == \"Thamnophis butleri\")\n",
    "# df_inat_species = df_train_meta_inat.where(F.col(\"binomial_name\") == \"Thamnophis butleri\")\n",
    "\n",
    "# df_hm_species.show(n=1, truncate=100, vertical=True)\n",
    "# df_inat_species.show(n=1, truncate=100, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"HM species count:   {df_hm_species.count()}\")\n",
    "# print(f\"iNat species count: {df_inat_species.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Base directory using pathlib\n",
    "# curr_dir = Path(os.getcwd())\n",
    "# base_dir = curr_dir.parents[1]\n",
    "# data_dir = base_dir / \"data\" / \"SnakeCLEF2023-small_size\"\n",
    "\n",
    "# # Ensure base directory exists\n",
    "# if not data_dir.is_dir():\n",
    "#     raise FileNotFoundError(f\"Data directory {data_dir} does not exist.\")\n",
    "\n",
    "# # Getting subfolders\n",
    "# folders = sorted([f.name for f in data_dir.iterdir() if f.is_dir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loop through subfolders\n",
    "# for folder in folders:\n",
    "#     folder_path = data_dir / folder\n",
    "#     subfolders = sorted([f.name for f in folder_path.iterdir() if f.is_dir()])\n",
    "#     for subfolder in subfolders:\n",
    "#         subfolder_path = folder_path / subfolder\n",
    "#         for img_name in os.listdir(subfolder_path):\n",
    "#             img_path = subfolder_path / img_name\n",
    "#             if f\"{subfolder}/{img_name}\" == \"Thamnophis_butleri/84733338.jpg\":\n",
    "#                 print(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get list of HM image paths\n",
    "# hm_list_rows = df_hm_species.select([\"image_path\"]).collect()\n",
    "# hm_list = [row[\"image_path\"] for row in hm_list_rows]\n",
    "# print(hm_list[:5])\n",
    "\n",
    "# # Get list of iNat image paths\n",
    "# inat_list_rows = df_inat_species.select([\"image_path\"]).collect()\n",
    "# inat_list = [row[\"image_path\"] for row in inat_list_rows]\n",
    "# print(inat_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if image exists in dataset\n",
    "# def check_images_exist(image_list:list) -> list:\n",
    "#     image_path_list = []\n",
    "#     for img in image_list:\n",
    "#         img_split = img.split(\"/\")\n",
    "#         img_final = f\"{img_split[1]}/{img_split[-1]}\"\n",
    "#         for folder in folders:\n",
    "#             folder_path = data_dir / folder\n",
    "#             subfolders = sorted([f.name for f in folder_path.iterdir() if f.is_dir()])\n",
    "#             if \"Thamnophis_butleri\" not in subfolders:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 for subfolder in subfolders:\n",
    "#                     subfolder_path = folder_path / subfolder\n",
    "#                     for img_name in os.listdir(subfolder_path):\n",
    "#                         img_path = subfolder_path / img_name\n",
    "#                         if f\"{subfolder}/{img_name}\" == img_final:\n",
    "#                             image_path_list.append(img_path)\n",
    "#     return image_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path_list = check_images_exist(image_list=inat_list)\n",
    "# len(image_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
