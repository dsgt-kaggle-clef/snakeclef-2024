{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image to Parquet\n",
    "\n",
    "Convert images to binary and save them into a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/26 22:08:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fa3784194e0>\n"
     ]
    }
   ],
   "source": [
    "import site\n",
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def get_gcs_connector_jar() -> str:\n",
    "    # Assuming the JAR is installed in the user site-packages of PySpark\n",
    "    user_site_packages = site.getusersitepackages()\n",
    "    jars_dir = Path(user_site_packages) / 'pyspark' / 'jars'\n",
    "    # Search for the GCS connector JAR\n",
    "    jar = [jar for jar in jars_dir.glob('gcs-connector-hadoop3-*.jar')]\n",
    "    return str(jar[0])\n",
    "\n",
    "gcs_connector_jar = get_gcs_connector_jar()\n",
    "# Initialize Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Image2Parquet\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.jars\", gcs_connector_jar)\n",
    "    .getOrCreate()\n",
    ")\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hadoop configurations for GCS\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Base directory using pathlib\n",
    "curr_dir = Path(os.getcwd())\n",
    "base_dir = curr_dir.parents[1]\n",
    "data_dir = base_dir / \"data\" / \"SnakeCLEF2023-small_size\" / \"2023\"\n",
    "num_folders = 20\n",
    "\n",
    "# Ensure base directory exists\n",
    "if not data_dir.is_dir():\n",
    "    raise FileNotFoundError(f\"Data directory {data_dir} does not exist.\")\n",
    "\n",
    "# Getting subfolders\n",
    "subfolders = sorted([f.name for f in data_dir.iterdir() if f.is_dir()])[:num_folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for the DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, BinaryType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"path\", StringType(), True),\n",
    "    StructField(\"folder_name\", StringType(), True),\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"binomial_name\", StringType(), True),\n",
    "    StructField(\"file_name\", StringType(), True),\n",
    "    StructField(\"image_binary_data\", BinaryType(), True)\n",
    "])\n",
    "\n",
    "# Function to convert image to binary\n",
    "def image_to_binary(image_path):\n",
    "    with open(image_path, 'rb') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty RDD\n",
    "image_rdd = spark.sparkContext.emptyRDD()\n",
    "\n",
    "# Loop through subfolders and process images\n",
    "for folder in subfolders:\n",
    "    folder_path = data_dir / folder\n",
    "    for img_name in os.listdir(folder_path):\n",
    "        img_path = folder_path / img_name\n",
    "        relative_path = img_path.relative_to(base_dir)  # Get relative path\n",
    "        relative_path = str(relative_path).split(\"data/\")[-1]\n",
    "        folder_name = relative_path.split(\"/\")[0]\n",
    "        year = relative_path.split(\"/\")[1]\n",
    "        binomial_name = relative_path.split(\"/\")[2]\n",
    "        file_name = relative_path.split(\"/\")[-1]\n",
    "        # print(f\"{folder_name}, {year}, {binomial_name}, {file_name}\")\n",
    "        binary_data = image_to_binary(str(img_path))\n",
    "        image_rdd = image_rdd.union(spark.sparkContext.parallelize([(\n",
    "            relative_path, folder_name, year,\n",
    "            binomial_name, file_name, binary_data,\n",
    "        )]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+--------------------+--------------+--------------------+\n",
      "|                path|         folder_name|year|       binomial_name|     file_name|   image_binary_data|\n",
      "+--------------------+--------------------+----+--------------------+--------------+--------------------+\n",
      "|SnakeCLEF2023-sma...|SnakeCLEF2023-sma...|2023|Acanthophis_antar...|250558438.jpeg|[FF D8 FF E0 00 1...|\n",
      "|SnakeCLEF2023-sma...|SnakeCLEF2023-sma...|2023|Acanthophis_antar...|250558444.jpeg|[FF D8 FF E0 00 1...|\n",
      "|SnakeCLEF2023-sma...|SnakeCLEF2023-sma...|2023|  Acanthophis_laevis|250489238.jpeg|[FF D8 FF E0 00 1...|\n",
      "|SnakeCLEF2023-sma...|SnakeCLEF2023-sma...|2023|Acanthophis_prael...| 252303073.jpg|[FF D8 FF E0 00 1...|\n",
      "|SnakeCLEF2023-sma...|SnakeCLEF2023-sma...|2023| Acanthophis_rugosus| 250956644.jpg|[FF D8 FF E0 00 1...|\n",
      "+--------------------+--------------------+----+--------------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert RDD to DataFrame\n",
    "image_df = spark.createDataFrame(image_rdd, schema)\n",
    "\n",
    "# Show the first few rows of image_df\n",
    "image_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory for parquet_files folder\n",
    "data_dir = Path(os.getcwd()).parents[1] / \"data\"\n",
    "\n",
    "# Create \"parquet_files\" directory if it doesn't exist\n",
    "parquet_dir = data_dir / \"parquet_files\"\n",
    "os.makedirs(parquet_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Path to the Parquet file\n",
    "parquet_file_path = parquet_dir / \"images_data.parquet\"\n",
    "\n",
    "# Save the DataFrame as a Parquet file\n",
    "image_df.write.mode(\"overwrite\").parquet(str(parquet_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- folder_name: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- binomial_name: string (nullable = true)\n",
      " |-- file_name: string (nullable = true)\n",
      " |-- image_binary_data: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Parquet file: 2979727 bytes\n"
     ]
    }
   ],
   "source": [
    "def get_size_of_parquet(dir_path):\n",
    "    total_size = 0\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            total_size += os.path.getsize(os.path.join(root, file))\n",
    "    return total_size\n",
    "\n",
    "# Get the size of the Parquet file (directory)\n",
    "parquet_size = get_size_of_parquet(parquet_file_path)\n",
    "print(f\"Size of Parquet file: {parquet_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the GCS path\n",
    "gcs_path = \"gs://dsgt-clef-snakeclef-2024/data/parquet_files/image_data\"\n",
    "\n",
    "# Write the DataFrame to GCS\n",
    "image_df.write.mode(\"overwrite\").parquet(gcs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
